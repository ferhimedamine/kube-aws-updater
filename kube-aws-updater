#!/bin/bash

# ## (-) master and worker steps
# ## (*) worker specific steps
#
# - label nodes
# - disable scheduling
# - double ASG size
# - wait for 2x group
# - stop ASG actions
#   * delete docker-reigistry pod
#   * wait for docker-registry to be up
# - drain/terminate labelled nodes (sequentially)
# - resize ASG to original size
# - re-enable ASG actions

set -e

. ./util.sh

RETIRE_TIME=$(date +"%Y-%m-%dT%H-%M-%SZ")

# flags
KUBE_CONTEXT=''
AWS_PROFILE=''
ROLE=''
REGISTRY_FLAG=''

while getopts 'c:p:r:e' flag; do
  case "${flag}" in
    c) KUBE_CONTEXT="${OPTARG}" ;;
    p) AWS_PROFILE="${OPTARG}" ;;
    r) ROLE="${OPTARG}" ;;
    e) REGISTRY_FLAG='true' ;;
    *) error "Unexpected option ${flag}" ;;
  esac
done

usage() {
  echo "Usage: ${0} -c <KUBE_CONTEXT> -p <AWS_PROFILE> -r [ROLE] -e [REGISTRY_FLAG]"
}

### Validation
type jq >/dev/null      || (log ${RED} "'jq' not found" && exit 1)
type kubectl >/dev/null || (log ${RED} "'kubectl' not found" && exit 1)
type aws >/dev/null     || (log ${RED} "'aws' not found" && exit 1)

if [[ ! -n "${KUBE_CONTEXT}" ]]; then
  usage
  exit 1
fi

if [[ -n "${AWS_PROFILE}" ]]; then
  log ${NC} "Using ${AWS_PROFILE} aws profile"
  AWS_PROFILE_CLI="--profile=${AWS_PROFILE}"
elif [[ -n "${AWS_ACCESS_KEY_ID}" && -n "${AWS_SECRET_ACCESS_KEY}" && -n "${AWS_REGION}" ]]; then
  log ${NC} "Using ${AWS_ACCESS_KEY_ID} aws keys"
  AWS_PROFILE_CLI=""
else
  log ${RED} "You need to set AWS_PROFILE or (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_REGION)"
  usage
  exit 1
fi
###

log ${GREEN} "Current Kubernetes cluster is ${KUBE_CONTEXT}"

handle_registry() {
  if [[ ${REGISTRY_FLAG} == 'true' ]]; then
    ns="sys-registry"
    registry_pod=$(kubectl --context=${KUBE_CONTEXT} -n${ns} get pod \
      | grep docker-registry \
      | awk '{print $1}')
    kubectl --context=${KUBE_CONTEXT} -n${ns} delete pod ${registry_pod}

    until kubectl --context=${KUBE_CONTEXT} -n${ns} get pod | grep docker-registry | grep Running; do
      kubectl --context=${KUBE_CONTEXT} -n${ns} get pod | grep docker-registry
    done
  fi
}

label_for_cycling() {
  role=${1}
  nodes=$(kubectl --context=${KUBE_CONTEXT} get nodes -l role=${role} -o json | jq -r '.items[].metadata.name')
  log ${NC} "${KUBE_CONTEXT} cluster has $(echo "${nodes}" | wc -l) ${role}s"
  log ${BLUE} "Labelling ${role}s for retirement..."
  while read -r node
  do
    kubectl --context=${KUBE_CONTEXT} label node ${node} retiring=${RETIRE_TIME} --overwrite=true
    kubectl --context=${KUBE_CONTEXT} cordon ${node}
  done <<< "${nodes}"
}

kill_node() {
  node=${1}

  log ${BLUE} "---------- ${node} ----------"
  log ${BLUE} "draining ${node}..."
  time kubectl --context=${KUBE_CONTEXT} drain ${node} --ignore-daemonsets --force --delete-local-data
  instance_id=$(aws ${AWS_PROFILE_CLI} --output=json ec2 describe-instances --filters "Name=network-interface.private-dns-name,Values=${node}" \
    | jq -r '.Reservations[].Instances[].InstanceId')
  log ${BLUE} "terminating ${node} with instance-id ${instance_id}"
  time aws ${AWS_PROFILE_CLI} ec2 terminate-instances --instance-ids=${instance_id}
  log ${BLUE} "----------------------------------------------------------------------"
}

cycle_nodes() {
  role=${1}

  # - double ASG size
  instance_address=$(kubectl --context=${KUBE_CONTEXT} get nodes -l role=${role},retiring=${RETIRE_TIME} -o json | jq -r '.items[0].metadata.name')
  instance_id=$(aws ${AWS_PROFILE_CLI} --output=json ec2 describe-instances --filters "Name=network-interface.private-dns-name,Values=${instance_address}" \
    | jq -r '.Reservations[].Instances[].InstanceId')
  asg_name=$(aws ${AWS_PROFILE_CLI} --output=json autoscaling describe-auto-scaling-groups \
    | jq -r '."AutoScalingGroups"[] | select(."Instances"[]."InstanceId"=="'${instance_id}'") | ."AutoScalingGroupName"')
  asg_count=$(aws ${AWS_PROFILE_CLI} --output=json autoscaling describe-auto-scaling-groups \
    | jq -r '."AutoScalingGroups"[] | select(."Instances"[]."InstanceId"=="'${instance_id}'") | ."DesiredCapacity"')
  aws ${AWS_PROFILE_CLI} --output=json autoscaling update-auto-scaling-group --auto-scaling-group-name "${asg_name}" --desired-capacity $(( ${asg_count} * 2)) --max-size $(( ${asg_count} * 2))

  # - wait for 2x group
  count=${asg_count}
  while [[ ${count} -lt $(( ${asg_count} * 2)) ]]
  do
    log ${BLUE} "${count}/$(( ${asg_count} * 2)) ready ${role}s"
    sleep 8
    count=$(kubectl --context=${KUBE_CONTEXT} get node -lrole=${role} | grep -v NotReady | tail -n +2 | wc -l)
  done

  # - stop ASG actions
  aws ${AWS_PROFILE_CLI} --output=json autoscaling suspend-processes --auto-scaling-group-name "${asg_name}"

  ### - move the registry
  if [[ ${role} == "worker" ]]; then
    handle_registry
  fi

  # - drain/terminate labelled masters (sequentially)
  old_nodes=$(kubectl --context=${KUBE_CONTEXT} get nodes -l role=${role},retiring=${RETIRE_TIME} -o json | jq -r '.items[].metadata.name')
  while read -r old_node
  do
    kill_node ${old_node}
  done <<< "${old_nodes}"

  # - resize ASG to original size
  aws ${AWS_PROFILE_CLI} --output=json autoscaling update-auto-scaling-group --auto-scaling-group-name "${asg_name}" --desired-capacity ${asg_count} --max-size ${asg_count}

  # - re-enable ASG actions
  aws ${AWS_PROFILE_CLI} --output=json autoscaling resume-processes --auto-scaling-group-name "${asg_name}"
}

update() {
  role=${1}
  label_for_cycling ${role}
  cycle_nodes ${role}
}

if [[ -n ${ROLE} ]]; then
  update ${ROLE}
else
  update master
  update worker
fi
